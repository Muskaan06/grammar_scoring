{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "z3zg0iol7H4n",
      "metadata": {
        "id": "z3zg0iol7H4n"
      },
      "source": [
        "# Grammar Scoring Competition\n",
        "\n",
        "## 1. Approach Overview\n",
        "The objective of this competition is to predict a grammar score, which is fundamentally a **regression problem**. The chosen approach involves conversion of provided audio files to transcripts using a pre-trained ASR model name Whisper. This was then fed as the fine-tuning training data to a transformer based model named DaBERT for grammar scoring task.\n",
        "\n",
        "## 2. Preprocessing Steps\n",
        "The initial preprocessing pipeline is designed to standardize the audio data before feature extraction:\n",
        "* **Package Installation:** All required libraries (`torchaudio`, `wordfreq`, etc.) are installed.\n",
        "* **Audio Standardization:** The core `load_and_resample` function ensures all raw audio files are loaded and resampled to a consistent rate of **16,000 Hz**.\n",
        "* **Mono Conversion:** Multi-channel audio (e.g., stereo) is converted to **mono** (single channel) by averaging the channels, which is standard practice for speech processing.\n",
        "\n",
        "\n",
        "## 3. Pipeline Architecture\n",
        "The machine learning pipeline follows a standard supervised learning flow:\n",
        "1.  **Raw Input:** Audio File + Ground Truth Score.\n",
        "2.  **Audio Preprocessing:** Resampling (16kHz) and Mono Conversion.\n",
        "3.  **Speech to text:** The audio was then converted to raw text using ASR model name **Whisper**. The generated texts were saved inform of csv.\n",
        "4.  **Model Training:** Training the **transformer** on the saved text csv.\n",
        "5.  **Evaluation:** Performance is assessed using **Root Mean Square Error (RMSE)**.\n",
        "## 4. Evaluation Results\n",
        "The final results from the model run are presented below.\n",
        "\n",
        "| Transformer model | Train RMSE | Leaderboard Score\n",
        "| :--- | :--- | :--- |\n",
        "| DaBERT |  0.16188851 | 0.600\n",
        "| DaBERT-small|  0.16698851 | 0.599\n",
        "| DaBERT-large |  0.1449279 | 0.735"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zj9E-Mk_v6YS",
      "metadata": {
        "collapsed": true,
        "id": "zj9E-Mk_v6YS"
      },
      "outputs": [],
      "source": [
        "!pip install wordfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pYjzfmKlxe6e",
      "metadata": {
        "collapsed": true,
        "id": "pYjzfmKlxe6e"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gIYKsJbVuOD7",
      "metadata": {
        "collapsed": true,
        "id": "gIYKsJbVuOD7"
      },
      "outputs": [],
      "source": [
        "!pip install TorchCodec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.57.1"
      ],
      "metadata": {
        "id": "fFvqLo4FpQNn"
      },
      "id": "fFvqLo4FpQNn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6jqLZjgOdfyI",
      "metadata": {
        "collapsed": true,
        "id": "6jqLZjgOdfyI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737a4213",
      "metadata": {
        "id": "737a4213"
      },
      "source": [
        "## Visualizing waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ab65ee",
      "metadata": {
        "id": "b4ab65ee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82ccf32",
      "metadata": {
        "id": "c82ccf32"
      },
      "outputs": [],
      "source": [
        "# Function to handle the actual plotting logic for waveform or spectrogram.\n",
        "def _plot(waveform, sample_rate, title):\n",
        "  \"\"\"\n",
        "  Internal helper function to plot the waveform or spectrogram.\n",
        "  Handles conversion to numpy and setting up the plot axes.\n",
        "  \"\"\"\n",
        "  # Convert PyTorch tensor to NumPy array for plotting\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  # Create a time axis based on the number of frames and sample rate\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    # Plot the waveform vs time for the Waveform visualization\n",
        "    if title == \"Waveform\":\n",
        "      axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "      axes[c].grid(True)\n",
        "    # Plot the spectrogram (frequency vs time)\n",
        "    else:\n",
        "      axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "# Public function to display the audio waveform (amplitude over time).\n",
        "def plot_waveform(waveform, sample_rate):\n",
        "  \"\"\"Plots the time-domain waveform of the audio signal.\"\"\"\n",
        "  _plot(waveform, sample_rate, title=\"Waveform\")\n",
        "\n",
        "# Public function to display the audio spectrogram (frequency content over time).\n",
        "def plot_specgram(waveform, sample_rate):\n",
        "  \"\"\"Plots the spectrogram of the audio signal (currently not used but included for completeness).\"\"\"\n",
        "  _plot(waveform, sample_rate, title=\"Spectrogram\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa8fb99b",
      "metadata": {
        "id": "fa8fb99b"
      },
      "outputs": [],
      "source": [
        "# Purpose: Ensure all audio files have the same format:\n",
        "# ✔ Same sample rate (e.g., 16kHz)\n",
        "# ✔ Converted to mono (1 channel)\n",
        "\n",
        "def load_and_resample(path, target_sr=16000):\n",
        "    # Load the audio file from the specified path, obtaining the waveform tensor and original sample rate (sr).\n",
        "    waveform, sr = torchaudio.load(path)  # shape: [channels, time]\n",
        "\n",
        "    # Check if the original sample rate (sr) matches the target rate.\n",
        "    if sr != target_sr:\n",
        "        # Initialize the Resample transform from torchaudio.\n",
        "        resampler = T.Resample(orig_freq=sr, new_freq=target_sr)\n",
        "        # Apply the resampling transformation to the waveform.\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono if the audio has multiple channels (e.g., stereo)\n",
        "    if waveform.shape[0] > 1:\n",
        "        # Average the channels along the first dimension to create a single mono channel\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    return waveform, target_sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08732ded",
      "metadata": {
        "id": "08732ded"
      },
      "outputs": [],
      "source": [
        "file_path = \"drive/MyDrive/grammar_scoring/audios/train/audio_1.wav\"\n",
        "waveform, sample = load_and_resample(file_path)\n",
        "plot_waveform(waveform, sample)\n",
        "waveform = waveform.squeeze() #always squeeze waveform to avoid dimension related errors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YUAZlQIvKrn4",
      "metadata": {
        "id": "YUAZlQIvKrn4"
      },
      "source": [
        "## Speech to Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hruXoU11XNeB",
      "metadata": {
        "id": "hruXoU11XNeB"
      },
      "source": [
        "### Whisper\n",
        "Loading and tesing Whisper model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jgnYfQIVXzX4",
      "metadata": {
        "id": "jgnYfQIVXzX4"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bK0WA0Ih0QdB",
      "metadata": {
        "id": "bK0WA0Ih0QdB"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hKIMmlk-XS0R",
      "metadata": {
        "id": "hKIMmlk-XS0R"
      },
      "outputs": [],
      "source": [
        "# # load model and processor\n",
        "\n",
        "#The Whisper model is intrinsically designed to work on audio samples of up to 30s in duration.\n",
        "#However, by using a chunking algorithm, it can be used to transcribe audio samples of up to arbitrary length.\n",
        "#This is possible through Transformers pipeline method. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline.\n",
        "\n",
        "pipe = pipeline(\n",
        "  \"automatic-speech-recognition\",\n",
        "  model=\"openai/whisper-medium\",\n",
        "  chunk_length_s=30,\n",
        "  stride_length_s=2,\n",
        "  device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3xz0Ygmdnyky",
      "metadata": {
        "id": "3xz0Ygmdnyky"
      },
      "outputs": [],
      "source": [
        "#this is the main function where transcripts are generated from audio\n",
        "def transcript(file_name):\n",
        "  file_path = \"drive/MyDrive/grammar_scoring/audios/train/\" + file_name\n",
        "  waveform, sample = load_and_resample(file_path)\n",
        "  waveform = waveform.squeeze()\n",
        "  # input_features = processor(waveform, sampling_rate=sample, return_tensors=\"pt\").input_features\n",
        "  # predicted_ids = model.generate(input_features)\n",
        "  # transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "  transcription = pipe(waveform, batch_size=8, return_timestamps=True)[\"text\"]\n",
        "  # print(transcription)\n",
        "  return transcription\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726JSJsHYG4u",
      "metadata": {
        "id": "726JSJsHYG4u"
      },
      "outputs": [],
      "source": [
        "# #testing with an audio file\n",
        "file_path = \"audio_1.wav\"\n",
        "print(transcript(file_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hzr9BNSJbcXa",
      "metadata": {
        "id": "hzr9BNSJbcXa"
      },
      "source": [
        "### Generating csv\n",
        "After loading and testing the pre-trained Whisper model, the audios from train dataset were now converted to their respective transcript which was saved in the form of csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CE1VZkgbnYgz",
      "metadata": {
        "id": "CE1VZkgbnYgz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kAXxQT6dooVd",
      "metadata": {
        "id": "kAXxQT6dooVd"
      },
      "outputs": [],
      "source": [
        "#creating dataframe\n",
        "df = pd.DataFrame(columns=[\"filename\", \"transcript\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PCDXWoyrbGy_",
      "metadata": {
        "id": "PCDXWoyrbGy_"
      },
      "outputs": [],
      "source": [
        "dir_path = \"drive/MyDrive/grammar_scoring/audios/train\"\n",
        "for files in os.listdir(dir_path):\n",
        "\n",
        "  name = os.path.splitext(os.path.basename(files))[0]\n",
        "  print(name)\n",
        "\n",
        "  # speech to audio convertion\n",
        "  trans = transcript(files)\n",
        "  print(trans)\n",
        "\n",
        "  df.loc[len(df)] = {'filename': name, 'transcript': trans}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YssnErOxuNEE",
      "metadata": {
        "id": "YssnErOxuNEE"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"drive/MyDrive/grammar_scoring/csvs/transcript_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ptK_c0HGaJV7",
      "metadata": {
        "id": "ptK_c0HGaJV7"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H8t-C8nSu5ep",
      "metadata": {
        "id": "H8t-C8nSu5ep"
      },
      "source": [
        "###data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D_3-__Fxzefl",
      "metadata": {
        "id": "D_3-__Fxzefl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-tN5mNINvnG9",
      "metadata": {
        "id": "-tN5mNINvnG9"
      },
      "outputs": [],
      "source": [
        "df_main = pd.read_csv(\"drive/MyDrive/grammar_scoring/csvs/train.csv\")\n",
        "df_train = pd.read_csv(\"drive/MyDrive/grammar_scoring/csvs/transcript_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yTLkcDX7l6QP",
      "metadata": {
        "id": "yTLkcDX7l6QP"
      },
      "outputs": [],
      "source": [
        "df_final = pd.merge(df_train, df_main, on=\"filename\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MGBkPtoaIj_H",
      "metadata": {
        "id": "MGBkPtoaIj_H"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "czJ49HQlK1v0",
      "metadata": {
        "id": "czJ49HQlK1v0"
      },
      "outputs": [],
      "source": [
        "# removing rows having non-english characters\n",
        "# Regex pattern allowing only English letters, digits, whitespace, and some punctuation\n",
        "import re\n",
        "pattern = re.compile(r'^[\\x00-\\x7F]*$')\n",
        "\n",
        "# Function to test each cell (convert to string to avoid errors)\n",
        "def is_clean(value):\n",
        "    return bool(pattern.match(str(value)))\n",
        "\n",
        "# Keep rows where **all columns** satisfy the condition\n",
        "clean_df = df_final[df_final.apply(lambda row: all(is_clean(x) for x in row), axis=1)]\n",
        "clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bY7Fpq676h2i",
      "metadata": {
        "id": "bY7Fpq676h2i"
      },
      "outputs": [],
      "source": [
        "clean_df = df_final.drop(columns=[\"filename\"])\n",
        "clean_df = clean_df.rename(columns={\"label\": \"labels\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MvjcPDqu7iLZ",
      "metadata": {
        "id": "MvjcPDqu7iLZ"
      },
      "source": [
        "## Transformer model for regression\n",
        "A pre-trained DaBERT model was fine-tuned as a regressor for grammar scoring task. General steps like loading and tokenizing were done as per HuggingFace documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4xfhS6tj7mVf",
      "metadata": {
        "id": "4xfhS6tj7mVf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, Value\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A93Hnac5w3Wp",
      "metadata": {
        "id": "A93Hnac5w3Wp"
      },
      "outputs": [],
      "source": [
        "dataset_train = Dataset.from_pandas(clean_df)\n",
        "dataset_train = dataset_train.cast_column(\"labels\", Value(\"float32\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wK2aSgCssUAB",
      "metadata": {
        "id": "wK2aSgCssUAB"
      },
      "outputs": [],
      "source": [
        "model_name = \"microsoft/deberta-v3-large\"   # recommended for grammar scoring\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_bcgxVHnshQm",
      "metadata": {
        "id": "_bcgxVHnshQm"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"transcript\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )\n",
        "\n",
        "train_ds = dataset_train.map(tokenize_fn, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Iw9gfeIosq1W",
      "metadata": {
        "id": "Iw9gfeIosq1W"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.remove_columns(\n",
        "    [col for col in train_ds.column_names if col not in [\"input_ids\",\"attention_mask\",\"labels\"]]\n",
        ")\n",
        "\n",
        "train_ds.set_format(type=\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DV1N59kXzCOf",
      "metadata": {
        "id": "DV1N59kXzCOf"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=1,                 # regression\n",
        "    problem_type=\"regression\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NFilq8GEzEQZ",
      "metadata": {
        "id": "NFilq8GEzEQZ"
      },
      "outputs": [],
      "source": [
        "mse = evaluate.load(\"mse\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.squeeze()\n",
        "    return {\"mse\": mse.compute(predictions=preds, references=labels)[\"mse\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J8e6U2eEzHqb",
      "metadata": {
        "id": "J8e6U2eEzHqb"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./grammar_model\",\n",
        "    num_train_epochs=40,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,          # optional\n",
        "    load_best_model_at_end=False,   # important: no eval → cannot pick “best”\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNlfoZcaQxPQ",
      "metadata": {
        "id": "MNlfoZcaQxPQ"
      },
      "outputs": [],
      "source": [
        "class RMSETrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Extract labels\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits.squeeze()\n",
        "\n",
        "        # MSE loss\n",
        "        mse = torch.nn.functional.mse_loss(logits, labels)\n",
        "\n",
        "        # RMSE = sqrt(MSE)\n",
        "        rmse = torch.sqrt(mse)\n",
        "\n",
        "        return (rmse, outputs) if return_outputs else rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jtw3AeehQwQk",
      "metadata": {
        "id": "Jtw3AeehQwQk"
      },
      "outputs": [],
      "source": [
        "trainer = RMSETrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1OzOGwHuscAC",
      "metadata": {
        "id": "1OzOGwHuscAC"
      },
      "outputs": [],
      "source": [
        "#epoch = 100\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tScOZn7pudqn",
      "metadata": {
        "id": "tScOZn7pudqn"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"drive/MyDrive/grammar_scoring/grammar_model_dabert_large4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dGRTYMGRqeaB",
      "metadata": {
        "id": "dGRTYMGRqeaB"
      },
      "source": [
        "## Full model pipeline for inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q5GlRm5Tqgo2",
      "metadata": {
        "collapsed": true,
        "id": "Q5GlRm5Tqgo2"
      },
      "outputs": [],
      "source": [
        "#step 1: Whisper for audio to text conversion\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio.functional as F\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#waveform load function\n",
        "def load_and_resample(path, target_sr=16000):\n",
        "    waveform, sr = torchaudio.load(path)  # shape: [channels, time]\n",
        "    if sr != target_sr:\n",
        "        resampler = T.Resample(orig_freq=sr, new_freq=target_sr)\n",
        "        waveform = resampler(waveform)\n",
        "    # convert to mono (average channels)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "    return waveform, target_sr\n",
        "\n",
        "#Whisper model defintion\n",
        "pipe = pipeline(\n",
        "  \"automatic-speech-recognition\",\n",
        "  model=\"openai/whisper-medium\",\n",
        "  chunk_length_s=30,\n",
        "  stride_length_s=2,\n",
        "  device=device\n",
        ")\n",
        "\n",
        "#audio to text function\n",
        "def transcript(file_name):\n",
        "  file_path = \"drive/MyDrive/grammar_scoring/audios/test/\" + file_name\n",
        "  waveform, sample = load_and_resample(file_path)\n",
        "  waveform = waveform.squeeze()\n",
        "  # input_features = processor(waveform, sampling_rate=sample, return_tensors=\"pt\").input_features\n",
        "  # predicted_ids = model.generate(input_features)\n",
        "  # transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "  transcription = pipe(waveform, batch_size=8, return_timestamps=True)[\"text\"]\n",
        "  # print(transcription)\n",
        "  return transcription\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZU6Lq2Oeq-ma",
      "metadata": {
        "id": "ZU6Lq2Oeq-ma"
      },
      "outputs": [],
      "source": [
        "# fine-tuned transformer(dabert) for text preprocessing and grammar scoring\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "model_name = \"/content/drive/MyDrive/grammar_scoring/grammar_model_dabert_large3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# def score_batch(sentences):\n",
        "#     inputs = tokenizer(\n",
        "#         sentences,\n",
        "#         return_tensors=\"pt\",\n",
        "#         padding=True,\n",
        "#         truncation=True,\n",
        "#         max_length=128,\n",
        "#     )\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "#     scores = outputs.logits.squeeze().tolist()\n",
        "#     return scores\n",
        "\n",
        "def score_text(sentence):\n",
        "    inputs = tokenizer(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    score = outputs.logits.squeeze().item()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dEP_VBh8tG9x",
      "metadata": {
        "id": "dEP_VBh8tG9x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"drive/MyDrive/grammar_scoring/audios/test\"\n",
        "test_df =  pd.DataFrame(columns=[\"filename\", \"label\"])\n",
        "\n",
        "for files in os.listdir(file_path):\n",
        "  name = os.path.splitext(os.path.basename(files))[0]\n",
        "  print(name)\n",
        "  # speech to audio convertion\n",
        "  trans = transcript(files)\n",
        "  print(trans)\n",
        "  score = score_text(trans)\n",
        "  print(score)\n",
        "\n",
        "  test_df.loc[len(test_df)] = {'filename': name, 'label': score}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wb5G1AR7Pj4S",
      "metadata": {
        "id": "wb5G1AR7Pj4S"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv(\"output4.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}